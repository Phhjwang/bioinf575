{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced libraries and Subprocesses\n",
    "### BIOINF 575"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced libraries\n",
    "#### scipy, scikit-learn, tensorflow, keras, pytorch\n",
    "\n",
    "It is important to practice using common advanced libraries:\n",
    "* https://dataheadhunters.com/academy/how-to-use-python-for-bioinformatics-detailed-step-by-step-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy\n",
    "https://scipy.org\n",
    "\n",
    "https://github.com/scipy/scipy\n",
    "\n",
    "Summary generated with perplexity AI:    \n",
    "https://www.perplexity.ai/search/you-are-a-teacher-in-grad-leve-540weGGrTa.oAUSrn8ecqg\n",
    "\n",
    "#### Overview of SciPy\n",
    "SciPy is a powerful open-source Python library designed for scientific and technical computing. It builds on the capabilities of the NumPy library, providing additional tools and algorithms for a wide range of mathematical and scientific calculations. SciPy is known for its efficient and user-friendly interfaces, making it a popular choice among scientists, engineers, and data analysts126.\n",
    "##### Key Features\n",
    "* Numerical Integration and Optimization: SciPy offers functions for numerical integration, interpolation, and optimization, which are essential for solving complex mathematical problems.\n",
    "* Statistics: The scipy.stats module provides a comprehensive suite of statistical functions, including probability distributions, statistical tests, and descriptive statistics.\n",
    "* Linear Algebra: SciPy includes advanced linear algebra capabilities, such as solving systems of equations, eigenvalue problems, and matrix decompositions14.\n",
    "Signal Processing: The library also supports signal processing tasks through its signal module, which includes tools for filtering, spectral analysis, and resampling.\n",
    "\n",
    "Table from:   \n",
    "https://lectures.scientific-python.org/intro/scipy/index.html\n",
    "\n",
    "\n",
    "| Module            | Description                            |\n",
    "|-------------------|----------------------------------------|\n",
    "| scipy.cluster     | Vector quantization / Kmeans           |\n",
    "| scipy.constants   | Physical and mathematical constants    |\n",
    "| scipy.fft         | Fourier transform                      |\n",
    "| scipy.integrate   | Integration routines                   |\n",
    "| scipy.interpolate | Interpolation                          |\n",
    "| scipy.io          | Data input and output                  |\n",
    "| scipy.linalg      | Linear algebra routines                |\n",
    "| scipy.ndimage     | n-dimensional image package            |\n",
    "| scipy.odr         | Orthogonal distance regression         |\n",
    "| scipy.optimize    | Optimization                           |\n",
    "| scipy.signal      | Signal processing                      |\n",
    "| scipy.sparse      | Sparse matrices                        |\n",
    "| scipy.spatial     | Spatial data structures and algorithms |\n",
    "| scipy.special     | Any special mathematical functions     |\n",
    "| scipy.stats       | Statistics                             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/8400382/python-pip-silent-install/8400396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "dist = sp.stats.norm(loc=0, scale=1)  # standard normal distribution\n",
    "sample = dist.rvs(size=100000)  # \"random variate sample\"\n",
    "plt.hist(sample, bins=50, density=True, label='normalized histogram')  \n",
    "x = np.linspace(-5, 5)\n",
    "plt.plot(x, dist.pdf(x), label='PDF')\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sp.stats.normaltest(sample)\n",
    "res.statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pooch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "face = sp.datasets.face(gray=True)\n",
    "\n",
    "# Shift, rotate and zoom it\n",
    "shifted_face = sp.ndimage.shift(face, (50, 50))\n",
    "shifted_face2 = sp.ndimage.shift(face, (50, 50), mode='nearest')\n",
    "rotated_face = sp.ndimage.rotate(face, 30)\n",
    "cropped_face = face[50:-50, 50:-50]\n",
    "zoomed_face = sp.ndimage.zoom(face, 2)\n",
    "zoomed_face.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(151)\n",
    "plt.imshow(shifted_face, cmap=plt.cm.gray)\n",
    "plt.subplot(152)\n",
    "plt.imshow(shifted_face2, cmap=plt.cm.gray)\n",
    "plt.subplot(153)\n",
    "plt.imshow(rotated_face, cmap=plt.cm.gray)\n",
    "plt.subplot(154)\n",
    "plt.imshow(cropped_face, cmap=plt.cm.gray)\n",
    "plt.subplot(155)\n",
    "plt.imshow(zoomed_face, cmap=plt.cm.gray)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scikit-learn\n",
    "\n",
    "https://scikit-learn.org/stable/\n",
    "\n",
    "https://github.com/scikit-learn/scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary generated with perplexity AI:    \n",
    "https://www.perplexity.ai/search/you-are-a-teacher-in-grad-leve-eLszD.e9QpOEC14u7goMRg#0\n",
    "\n",
    "#### Overview of Scikit-learn\n",
    "Scikit-learn, often referred to as sklearn, is a widely-used open-source machine learning library for Python. It provides a range of supervised and unsupervised learning algorithms through a consistent interface in Python. The library is built on top of NumPy, SciPy, and Matplotlib, making it efficient for data analysis and modeling124.\n",
    "\n",
    "#### Key Features\n",
    "* Classification: Algorithms for categorizing data into predefined classes, such as logistic regression and support vector machines14.\n",
    "Regression: Techniques for predicting continuous values, like linear regression and decision tree regression14.\n",
    "Clustering: Methods for grouping similar data points, including k-means and DBSCAN14.\n",
    "* Dimensionality Reduction: Tools like Principal Component Analysis (PCA) to reduce the number of variables in data14.\n",
    "Model Selection and Evaluation: Functions to compare, validate, and choose models and their parameters1.\n",
    "Applications in Bioinformatics\n",
    "In bioinformatics, scikit-learn is particularly useful due to its versatility in handling various types of biological data. Here are some specific applications:\n",
    "* Gene Expression Analysis\n",
    "Classification: Used to classify gene expression profiles to identify different types of cancer or other diseases. Algorithms like support vector machines (SVM) are commonly applied for this purpose.\n",
    "Protein Structure Prediction\n",
    "Regression Models: Employed to predict protein structures based on amino acid sequences. Linear regression can be used to model relationships between sequence features and structural properties.\n",
    "* Clustering Biological Data\n",
    "Clustering Techniques: Useful for grouping similar gene expression profiles or protein structures. K-means clustering helps in identifying patterns within large datasets.\n",
    "* Dimensionality Reduction\n",
    "PCA and Other Techniques: These are used to reduce the dimensionality of complex biological datasets, making it easier to visualize and interpret the data.\n",
    "\n",
    "Scikit-learn's ability to integrate with other Python libraries such as NumPy, Pandas, and Matplotlib allows for seamless preprocessing, analysis, and visualization of bioinformatics data125. Its user-friendly API makes it accessible for researchers looking to apply machine learning techniques without extensive programming expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of clustering of digits\n",
    "# This dataset contains handwritten digits from 0 to 9. We would like to group images such that the handwritten digits on the image are the same.\n",
    "# Example from:\n",
    "# https://scikit-learn.org/1.5/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More examples at: \n",
    "# https://github.com/glouppe/tutorial-sklearn-lhcb/blob/master/An%20introduction%20to%20Machine%20Learning%20with%20Scikit-Learn-Rendered.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data, labels = load_digits(return_X_y=True)\n",
    "(n_samples, n_features), n_digits = data.shape, np.unique(labels).size\n",
    "\n",
    "print(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def bench_k_means(kmeans, name, data, labels):\n",
    "    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kmeans : KMeans instance\n",
    "        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n",
    "        already set.\n",
    "    name : str\n",
    "        Name given to the strategy. It will be used to show the results in a\n",
    "        table.\n",
    "    data : ndarray of shape (n_samples, n_features)\n",
    "        The data to cluster.\n",
    "    labels : ndarray of shape (n_samples,)\n",
    "        The labels used to compute the clustering metrics which requires some\n",
    "        supervision.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n",
    "    fit_time = time() - t0\n",
    "    results = [name, fit_time, estimator[-1].inertia_]\n",
    "\n",
    "    # Define the metrics which require only the true labels and estimator\n",
    "    # labels\n",
    "    clustering_metrics = [\n",
    "        metrics.homogeneity_score,\n",
    "        metrics.completeness_score,\n",
    "        metrics.v_measure_score,\n",
    "        metrics.adjusted_rand_score,\n",
    "        metrics.adjusted_mutual_info_score,\n",
    "    ]\n",
    "    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n",
    "\n",
    "    # The silhouette score requires the full dataset\n",
    "    results += [\n",
    "        metrics.silhouette_score(\n",
    "            data,\n",
    "            estimator[-1].labels_,\n",
    "            metric=\"euclidean\",\n",
    "            sample_size=300,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Show the results\n",
    "    formatter_result = (\n",
    "        \"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n",
    "    )\n",
    "    print(formatter_result.format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(82 * \"_\")\n",
    "print(\"init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette\")\n",
    "\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=4, random_state=0)\n",
    "bench_k_means(kmeans=kmeans, name=\"k-means++\", data=data, labels=labels)\n",
    "\n",
    "kmeans = KMeans(init=\"random\", n_clusters=n_digits, n_init=4, random_state=0)\n",
    "bench_k_means(kmeans=kmeans, name=\"random\", data=data, labels=labels)\n",
    "\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "kmeans = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)\n",
    "bench_k_means(kmeans=kmeans, name=\"PCA-based\", data=data, labels=labels)\n",
    "\n",
    "print(82 * \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=4)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(\n",
    "    Z,\n",
    "    interpolation=\"nearest\",\n",
    "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "    cmap=plt.cm.Paired,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(\n",
    "    centroids[:, 0],\n",
    "    centroids[:, 1],\n",
    "    marker=\"x\",\n",
    "    s=169,\n",
    "    linewidths=3,\n",
    "    color=\"w\",\n",
    "    zorder=10,\n",
    ")\n",
    "plt.title(\n",
    "    \"K-means clustering on the digits dataset (PCA-reduced data)\\n\"\n",
    "    \"Centroids are marked with white cross\"\n",
    ")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow\n",
    "\n",
    "https://www.tensorflow.org\n",
    "\n",
    "https://github.com/tensorflow/tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary generated with perplexity AI:    \n",
    "https://www.perplexity.ai/search/you-are-a-teacher-in-grad-leve-vrp1AdYmQl6NaeCh7428Eg#0\n",
    "\n",
    "#### Overview of TensorFlow\n",
    "TensorFlow is an open-source library developed by Google for numerical computation and large-scale machine learning, particularly deep learning applications. It allows developers to build and train models using a comprehensive ecosystem of tools and libraries. TensorFlow supports various machine learning tasks, including image recognition, natural language processing, and more, by utilizing dataflow graphs where nodes represent mathematical operations and edges represent multidimensional data arrays called tensors136.\n",
    "\n",
    "#### Key Features of TensorFlow\n",
    "* Dataflow Graphs: TensorFlow uses dataflow graphs to represent computations. This allows for efficient execution across different hardware platforms, including CPUs, GPUs, and TPUs34.\n",
    "* High-Level APIs: TensorFlow provides high-level APIs that simplify model building and training, making it accessible even to those not deeply familiar with the underlying algorithms14.\n",
    "* Cross-Platform Support: Models built with TensorFlow can be deployed on various platforms such as desktops, mobile devices, and cloud environments46.\n",
    "* Abstraction: The library abstracts complex operations, allowing developers to focus on the logic of their applications without delving into the intricacies of algorithm implementation3.\n",
    "\n",
    "##### Applications in Bioinformatics\n",
    "TensorFlow's capabilities are particularly useful in bioinformatics for handling large datasets and complex models. Here are some ways it is applied in this field:\n",
    "* Genomic Data Analysis: TensorFlow can process large genomic datasets to identify patterns and make predictions about genetic traits or disease predispositions.\n",
    "* Protein Structure Prediction: Deep learning models built with TensorFlow can predict protein structures based on amino acid sequences, aiding in drug discovery and development.\n",
    "* Medical Imaging: TensorFlow's image recognition capabilities are leveraged for analyzing medical images such as MRI or CT scans to detect anomalies or diseases.\n",
    "* Natural Language Processing (NLP): In bioinformatics, NLP models can process scientific literature or patient records to extract meaningful information for research or clinical decision-making.\n",
    "\n",
    "#### Advantages for Bioinformatics\n",
    "* Scalability: TensorFlow's ability to handle large-scale computations makes it ideal for bioinformatics tasks that require processing vast amounts of biological data16.\n",
    "* Flexibility: The library's support for various machine learning models allows researchers to experiment with different approaches to solve bioinformatics problems.\n",
    "* Community and Resources: Being widely used, TensorFlow has a strong community and a wealth of resources that can aid bioinformaticians in developing robust solutions3.\n",
    "\n",
    "In summary, TensorFlow is a powerful tool in the bioinformatics domain due to its scalability, flexibility, and comprehensive ecosystem that supports a wide range of applications from genomic analysis to medical imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary generated with perplexity AI:    \n",
    "https://www.perplexity.ai/search/you-are-a-teacher-in-grad-leve-3Rzgk7IoTfCkvSQdcl4B9Q#0\n",
    "\n",
    "#### Overview of Keras\n",
    "Keras is a high-level, deep learning API written in Python and developed by Google. It is designed to simplify the implementation of neural networks by providing a user-friendly and modular interface. Keras operates on top of several backend engines, including TensorFlow, Theano, MXNet, and CNTK, with TensorFlow being its most common backend due to its integration as the official high-level API123.\n",
    "\n",
    "##### Key Features\n",
    "* Ease of Use: Keras offers a simple and consistent API that reduces the complexity of implementing deep learning models. This makes it accessible for both beginners and experts15.\n",
    "* Modularity: The library is modular, allowing users to create complex architectures using the Sequential API or the Functional API, which supports arbitrary graphs of layers45.\n",
    "* Cross-Platform Compatibility: Keras models can run on various hardware configurations, including CPUs and GPUs, and can be exported for use across different platforms45.\n",
    "* Rapid Prototyping: The high-level abstractions in Keras facilitate quick iteration on ideas, making it suitable for fast experimentation25.\n",
    "\n",
    "#### Applications in Bioinformatics\n",
    "Keras is extensively used in bioinformatics for tasks such as genomics, proteomics, and medical imaging. Here are some specific applications:\n",
    "\n",
    "##### Genomics\n",
    "* Sequence Classification: Keras can be used to classify DNA sequences by building models that predict functional regions or detect mutations.\n",
    "* Gene Expression Analysis: Deep learning models in Keras can analyze gene expression data to identify patterns associated with diseases.\n",
    "\n",
    "##### Proteomics\n",
    "* Protein Structure Prediction: Keras models help predict protein structures from amino acid sequences using techniques like convolutional neural networks (CNNs).\n",
    "* Protein Function Prediction: By leveraging large datasets of protein sequences, Keras can assist in predicting protein functions based on structural and sequence data.\n",
    "\n",
    "##### Medical Imaging\n",
    "* Image Classification: Keras is used to classify medical images such as MRI or CT scans for diagnostic purposes.\n",
    "* Segmentation Tasks: In tasks like tumor detection, Keras models can segment medical images to highlight areas of interest.\n",
    "\n",
    "#### Advantages in Bioinformatics\n",
    "* Integration with TensorFlow: This allows for scalability and efficient computation on large datasets typical in bioinformatics.\n",
    "* Preprocessing Capabilities: Keras provides preprocessing layers that facilitate handling complex bioinformatics data types like sequences and images24.\n",
    "* Community Support: A robust community and extensive documentation support bioinformatics researchers in implementing cutting-edge models quickly15.\n",
    "\n",
    "Keras's simplicity, flexibility, and integration with powerful backends make it a valuable tool for advancing research and applications in bioinformatics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from:\n",
    "# https://www.tensorflow.org/tutorials/quickstart/beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model(x_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytorch   \n",
    "https://pytorch.org\n",
    "\n",
    "https://github.com/pytorch/pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary generated with perplexity AI:   \n",
    "https://www.perplexity.ai/search/you-are-a-teacher-in-grad-leve-urfuUUoZT1.IY8zNfEfHIQ#0\n",
    "\n",
    "\n",
    "#### Overview of PyTorch\n",
    "PyTorch is a widely-used open-source machine learning library developed by Meta AI, now part of the Linux Foundation. It is designed for applications such as computer vision and natural language processing (NLP) and is known for its flexibility and ease of use in building deep learning models. PyTorch offers a dynamic computational graph, which allows for more intuitive model building and debugging compared to static graph frameworks like TensorFlow.\n",
    "\n",
    "#### Key Features\n",
    "* Tensor Computing: PyTorch provides tensor computing similar to NumPy, with strong acceleration via GPUs, making it suitable for high-performance scientific computing2.\n",
    "* Deep Neural Networks: The library supports the creation of deep neural networks through its torch.nn module, which includes various layers and activation functions2.\n",
    "* Automatic Differentiation: PyTorch uses a tape-based automatic differentiation system that simplifies the process of computing gradients, essential for training neural networks.\n",
    "\n",
    "#### Applications in Bioinformatics\n",
    "In bioinformatics, PyTorch is increasingly used due to its flexibility and powerful features that support complex data analysis and model development. Here are some specific applications:\n",
    "##### Genomic Data Analysis\n",
    "* Sequence Analysis: PyTorch can be used to build models that analyze DNA sequences, predicting gene expression or identifying mutations. The dynamic computational graph allows researchers to experiment with different architectures easily.\n",
    "* Protein Structure Prediction: Deep learning models in PyTorch can predict protein structures from amino acid sequences, aiding in understanding protein functions and interactions.\n",
    "##### Imaging in Bioinformatics\n",
    "* Microscopy Image Analysis: PyTorch's capabilities in computer vision make it suitable for analyzing microscopy images, such as identifying cellular structures or quantifying biological markers.\n",
    "* Medical Imaging: It is also used in processing medical images like MRI or CT scans to detect anomalies or classify diseases.\n",
    "\n",
    "#### Integration with Other Tools\n",
    "PyTorch integrates well with other bioinformatics tools and libraries. For instance, it can be combined with libraries like scikit-learn for preprocessing or post-processing tasks. Additionally, its compatibility with cloud platforms facilitates large-scale data processing and model training5.\n",
    "\n",
    "#### Advantages in Bioinformatics\n",
    "* Flexibility: The dynamic nature of PyTorch allows bioinformaticians to modify models on-the-fly, which is crucial when dealing with complex biological data.\n",
    "* Community and Ecosystem: A robust community supports the development of specialized bioinformatics tools within the PyTorch ecosystem, enhancing collaborative research efforts.\n",
    "* \n",
    "In summary, PyTorch's adaptability and comprehensive feature set make it an excellent choice for bioinformatics applications, supporting a wide range of tasks from sequence analysis to medical imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples\n",
    "# Example from:\n",
    "# https://pytorch.org/tutorials/beginner/basics/intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subprocesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest strengths of Python is that it can be used as a *glue* language. <br>\n",
    "It can 'glue' together a series of programs into a flexible and highly extensible pipline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why subprocesses\n",
    "One of the most common, yet complicated, tasks that most programming languages need to do is creating new processes. <br>\n",
    "This could be as simple as seeing what files are present in the current working directory (`ls`) or as complicated as creating a program workflow that *pipes* output from one program into another program's input. <br/><br/>\n",
    "Many such tasks are easily taken care of through the use of Python libraries and modules (`import`) that *wrap* the programs into Python code, effectively creating Application Programming Interfaces (API). <br/><br/>\n",
    "However, there are many use cases that require the user to make calls to the terminal from ***within*** a Python program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operating System Conundrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to address the following issue. As many in this class have found out, while Python can be installed on most operating systems; doing the same thing in one operating system (Unix) may not always yield the same results in another (Windows).<br/><br/>\n",
    "The very first step to making a program **\"OS-agnostic\"** is through the use of the `os` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/os.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in dir(os):\n",
    "    if \"error\" in elem:\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the operating system dependent module imported. \n",
    "# The following names have currently been registered: 'posix', 'nt', 'java'\n",
    "# Portable Operating System Interface -  IEEE standard designed to facilitate application portability\n",
    "# (Windows) New Technology - a 32-bit operating system that supports preemptive multitasking\n",
    "# \n",
    "os.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns information identifying the current operating system. The return value is an object with five attributes:\n",
    "# - sysname - operating system name\n",
    "# - nodename - name of machine on network (implementation-defined)\n",
    "# - release - operating system release\n",
    "# - version - operating system version\n",
    "# - machine - hardware identifier\n",
    "\n",
    "os.uname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# https://docs.python.org/3/library/sys.html\n",
    "# This string contains a platform identifier that can be used to append platform-specific components\n",
    "# to sys.path, for instance.\n",
    "    \n",
    "sys.platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of strings that specifies the search path for modules. \n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A mapping object representing the string environment.\n",
    "\n",
    "os.environ['HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the value of the environment variable key if it exists, \n",
    "#or default if it doesnâ€™t. key, default and the result are str.\n",
    "\n",
    "os.getenv(\"HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the list of directories that will be searched for a named executable,\n",
    "#similar to a shell, when launching a process. \n",
    "# env, when specified, should be an environment variable dictionary to lookup the PATH in. \n",
    "# By default, when env is None, environ is used.\n",
    "\n",
    "os.get_exec_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `os` module wraps OS-specific operations into a set of standardized commands. <br>\n",
    "For instance, the Linux end-of-line (EOL) character is a `\\n`, but `\\r\\n` in Windows. <br>\n",
    "In Python, we can just use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOL - for the current (detected) environment\n",
    "\n",
    "'''\n",
    "The string used to separate (or, rather, terminate) lines on the current platform. \n",
    "This may be a single character, such as '\\n' for POSIX, or multiple characters, \n",
    "for example, '\\r\\n' for Windows. \n",
    "Do not use os.linesep as a line terminator when writing files opened in text mode (the default); \n",
    "use a single '\\n' instead, on all platforms.\n",
    "'''\n",
    "\n",
    "os.linesep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example, in a Linux environment, one must use the following command to list the contents of a given directory:\n",
    "```\n",
    "ls -alh \n",
    "```\n",
    "\n",
    "In Windows, the equivalent is as follows:\n",
    "```\n",
    "dir\n",
    "```\n",
    "\n",
    "Python allows users to do a single command, in spite of the OS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List directory contents\n",
    "\n",
    "os.listdir(\"demoCM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the biggest issue for creating an OS-agnostic program is ***paths*** <br/>\n",
    "Windows: `\"C:\\\\Users\\\\MDS\\\\Documents\"`<br/>\n",
    "Linux: `/mnt/c/Users/MDS/Documents/`<br/><br/>\n",
    "Enter Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path joining from pwd\n",
    "pwd = os.getcwd()\n",
    "os.path.join(pwd,\"test.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `subprocess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you Google anything on how to run shell commands, but don't specify Python 3.x, you will likely get an answer that includes `popen`, `popen2`, or `popen3`. These were the most prolific ways to *open* a new *p*rocess. In Python 3.x, they encapsulated these functions into a new one called `run` available through the `subprocess` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and alias\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `check_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_output returns a bytestring by default, so I set encoding to convert it to strings.\n",
    "# [command, command line arguments]\n",
    "# change from bytes to string using encoding\n",
    "\n",
    "sp.check_output([\"echo\",\"test\"],encoding='utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.check_output([os.path.join(pwd,\"test.py\"),\"[1,2,3]\"],encoding='utf_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we will look at are trivial examples that demonstrate just capturing the *output* (stdout) of a program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while the `check_output` function is still in the `subprocess` module, it can easily be converted into into a more specific and/or flexible `run` function signature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sp.run(\n",
    "    [\n",
    "        'echo',             # The command we want to run\n",
    "        'test'              # Arguments for the command\n",
    "    ],\n",
    "    encoding='utf_8',       # Converting byte code\n",
    "    stdout=sp.PIPE,         # Where to send the output\n",
    "    check=True              # Whether to raise an error if the process fails\n",
    ")  \n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sub.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main utility of `check_output` was to capture the output (stdout) of a program. <br>\n",
    "By using the `stdout=subprocess.PIPE` argument, the output can easily be captured, along with its return code. <br>\n",
    "A return code signifies the program's exit status: 0 for success, anything else otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.returncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our `run` code above, our program ran to completetion, exiting with status 0. The next example shows a different status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.run(\n",
    "        'exit 1',      # Command & arguments\n",
    "        shell = True   # Run from the shell\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the `check=True` argument is used, it will raise a `CalledProcessError` if your program exits with anything different than 0. This is helpful for detecting a pipeline failure, and exiting or correcting before attempting to continue computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.run(\n",
    "        'exit 1',      # Command & arguments\n",
    "        shell = True,  # Run from the shell\n",
    "        check = True   # Check exit status\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sp.run(\n",
    "        'exit 1',      # Command & arguments\n",
    "        shell = True,  # Run from the shell\n",
    "        # check = True   # Check exit status\n",
    "    )\n",
    "if (sub.returncode != 0):\n",
    "    print(f\"Exit code {sub.returncode}. Expected 0 when there is no error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax when using `run`:<br/>\n",
    "1. A list of arguments: `subprocess.run(['echo', 'test', ...], ...)` \n",
    "2. A string and `shell`: `subprocess.run('exit 1', shell = True, ...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preferred way of using `run` is the first way. <br>\n",
    "This preference is mainly due to security purposes (to prevent shell injection attacks). <br>\n",
    "It also allows the module to take care of any required escaping and quoting of arguments for a pseudo-OS-agnostic approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some guidelines though:\n",
    "1. Sequence (list) of arguments is generally preferred\n",
    "2. A str is appropriate if the user is just calling a program with no arguments\n",
    "3. The user should use a str to pass argument if `shell` is `True`<br/>\n",
    "Your next questions should be, \"What is `shell`?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`shell` is just your terminal/command prompt. This is the environment where you call `ls/dir` in. It is also where users can define variables. More importantly, this is where your *environmental variables* are set...like `PATH`.<br/><br/>\n",
    "By using `shell = True`, the user can now use shell-based environmental variable expansion from within a Python program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.run(\n",
    "        'echo $PATH',            # Command\n",
    "        shell = True,            # Use the shell\n",
    "        stdout=sp.PIPE,          # Where to send it\n",
    "        encoding='utf_8'         # Convert from bytes to string\n",
    "    )      # Look at the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = sp.run(\n",
    "        'sleep 5',               # Command\n",
    "        shell = True,            # Use the shell\n",
    "        stdout=sp.PIPE,          # Where to send it\n",
    "        encoding='utf_8'         # Convert from bytes to string\n",
    "    )\n",
    "print(p1)\n",
    "p2 = sp.run(\n",
    "        'echo done',             # Command\n",
    "        shell = True,            # Use the shell\n",
    "        stdout=sp.PIPE,          # Where to send it\n",
    "        encoding='utf_8'         # Convert from bytes to string\n",
    "    )\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, you shouldn't need to use `shell` simply because Python has modules in the standard library that can do most of the shell commands. For example `mkdir` can be done with `os.mkdir()`, and `$PATH` can be retrieved using os.getenv(\"PATH\") or os.get_exec_path() as shown above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blocking vs Non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last topic of this lecture is \"blocking\". This is computer science lingo/jargon for whether or not a program ***waits*** until something is complete before moving on. Think of this like a really bad website that takes forever to load because it is waiting until it has rendered all its images first, versus the website that sets the formatting and text while it works on the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `subprocess.run()` is blocking (it waits until the process is complete)\n",
    "2. `subprocess.Popen()` is non-blocking (it will run the command, then move on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Most*** use cases can be handled through the use of `run()`.<br> \n",
    "`run()` is just a *wrapped* version of `Popen()` that simplifies use. <br>\n",
    "However, `Popen()` allows the user a more flexible control of the subprocess call. <br>\n",
    "`Popen()` can be used similar way as run (with more optional parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example use case for `Popen()` is if the user has some intermediate data that needs to get processed, but the output of that data doesn't necessarily affect the rest of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Popen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = sp.Popen(\n",
    "        'sleep 5',               # Command\n",
    "        shell = True,            # Use the shell\n",
    "        stdout=sp.PIPE,          # Where to send it\n",
    "        encoding='utf_8'         # Convert from bytes to string\n",
    "    )\n",
    "print(p1)\n",
    "p2 = sp.Popen(\n",
    "        'echo done',             # Command\n",
    "        shell = True,            # Use the shell\n",
    "        stdout=sp.PIPE,          # Where to send it\n",
    "        encoding='utf_8'         # Convert from bytes to string\n",
    "    )\n",
    "print(p2)\n",
    "print(\"processes ran\")\n",
    "\n",
    "print(p1.stdout.read())\n",
    "print(p2.stdout.read())\n",
    "print(\"processes completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use context manager to handle process while it is running,\n",
    "# and gracefully close it\n",
    "with sp.Popen(\n",
    "    [\n",
    "        'echo',         # Command\n",
    "        'here we are'       # Command line arguments\n",
    "    ],\n",
    "    encoding='utf_8', # Convert from byte to string\n",
    "    stdout=sp.PIPE    # Where to send it\n",
    ") as proc:            # Enclose and alias the context manager\n",
    "    print(\n",
    "        proc.stdout.read() # Look at the output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in dir(proc):\n",
    "    if not elem.startswith('_'):\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***NOTE***: From here on out, there might be different commands used for **Linux** / **MacOS** or **Windows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following text to a new file `test_pipe.txt` \n",
    "```\n",
    "testing\n",
    "a\n",
    "subprocess\n",
    "pipe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to add the text to the file\n",
    "#test_pipe.txt - a file to be used to demonstrate pipe of cat and sort \n",
    "!echo testing > test_pipe.txt\n",
    "!echo a >> test_pipe.txt\n",
    "!echo subprocess >> test_pipe.txt\n",
    "!echo pipe >> test_pipe.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the first process - cat - reading the file content\n",
    "\n",
    "# mac OS\n",
    "p1 = sp.Popen(['cat','test_pipe.txt'], stdout=sp.PIPE, encoding='utf_8')\n",
    "\n",
    "# windows OS\n",
    "# p1 = sp.Popen(['type','test_pipe.txt'], stdout=sp.PIPE, encoding='utf_8')\n",
    "\n",
    "print(p1.stdout.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the second process and connect the pipe: \n",
    "# for p2 we use stdin=p1.stdout\n",
    "\n",
    "# mac OS\n",
    "p1 = sp.Popen(['cat','test_pipe.txt'], stdout=sp.PIPE, encoding='utf_8')\n",
    "\n",
    "# windows OS\n",
    "# p1 = sp.Popen(['type','test_pipe.txt'], stdout=sp.PIPE, encoding='utf_8')\n",
    "\n",
    "\n",
    "p2 = sp.Popen(['sort'], stdin=p1.stdout, stdout=sp.PIPE, encoding='utf_8')\n",
    "p1.stdout.close()  # Allow p1 to receive a SIGPIPE if p2 exits\n",
    "output = p2.communicate()[0]\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Popen` can create background processes, shell-background-like behavior means not blocking. <br>\n",
    "`Popen` has a lot more functionality than `run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_popen = sp.Popen(\n",
    "    [\n",
    "        'echo',          # Command\n",
    "        'test',        # Command line arguments\n",
    "    ],\n",
    "    encoding='utf_8',  # Convert from byte to string\n",
    "    stdout=sp.PIPE     # Where to send it\n",
    ")\n",
    "for j in dir(sub_popen):\n",
    "    if not j.startswith('_'):\n",
    "        print(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_popen.kill()       # Close the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example creating child process.<br>\n",
    "https://pymotw.com/3/subprocess/\n",
    "\n",
    "A collection of `Popen` examples: <br>\n",
    "https://www.programcreek.com/python/example/50/subprocess.Popen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "Write bash script that takes the file as an argument and returns lines that contain the letter p.    \n",
    "Call that script from python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise -  only if you have R installed\n",
    "`test.R` - R script that takes the file `test_R.txt` as an argument and returns the sum of the matrix from the file\n",
    "`Rscript` - the executable/interpreter used to run the R script\n",
    "\n",
    "```\n",
    "rN\tval1\tval2\n",
    "r1\t1\t2\n",
    "r2\t3\t4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
